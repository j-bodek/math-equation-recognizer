{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24760be9-745a-4d95-9298-d67963961f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fded525-4239-4457-ab88-fd688cf83c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {'*': 0, '+': 1, '-': 2, '/': 3, '0': 4, '1': 5, '2': 6, '3': 7, '4': 8, '5': 9, '6': 10, '7': 11, '8': 12, '9': 13, '=': 14}\n",
    "rev_classes = {v:k for k, v in classes.items()}\n",
    "n_classes = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5391d23-1f7c-4196-a056-ed1a58724a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"..models/classifier.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "745d41c3-ddb0-4ecc-8d46-98ada07728f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(img):\n",
    "    size = (28, 28)\n",
    "    img = cv2.resize(img, dsize=size, interpolation=cv2.INTER_CUBIC)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = img.astype(\"float32\")\n",
    "    img /= 255\n",
    "    img = img.reshape(1, 28*28)\n",
    "\n",
    "    prediction = model.predict(img)\n",
    "    prediction = prediction[0].argmax(axis=-1)\n",
    "    return rev_classes[prediction]\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04729c02-c060-4be4-9056-2d20193c8fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n"
     ]
    }
   ],
   "source": [
    "im = cv2.imread('eq.png')\n",
    "height, width, _ = im.shape\n",
    "im_copy = im.copy()\n",
    "\n",
    "gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "thresh = cv2.adaptiveThreshold(blur, 255, 1, 1, 11, 2)\n",
    "# detections\n",
    "contours, hierarchy = cv2.findContours(thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "bboxes = sorted([cv2.boundingRect(c) for c in contours], key=lambda x: (x[0], x[0]+x[2]))\n",
    "\n",
    "def merge_bboxes(bbox1, bbox2):\n",
    "    x = min(bbox1[0], bbox2[0])\n",
    "    y = min(bbox1[1], bbox2[1])\n",
    "    w = max(bbox1[0]+bbox1[2], bbox2[0]+bbox2[2]) - x\n",
    "    h = max(bbox1[1]+bbox1[3], bbox2[1]+bbox2[3]) - y\n",
    "    return [x, y, w, h]\n",
    "\n",
    "normalized_bboxes = []\n",
    "\n",
    "for bbox in bboxes:\n",
    "    if not normalized_bboxes:\n",
    "        normalized_bboxes.append(bbox)\n",
    "        continue\n",
    "\n",
    "    prev = normalized_bboxes[-1]\n",
    "    if prev[0]+prev[2] >= bbox[0]:\n",
    "        normalized_bboxes[len(normalized_bboxes)-1] = merge_bboxes(prev, bbox)\n",
    "    else:\n",
    "        normalized_bboxes.append(bbox)\n",
    "    \n",
    "eq = \"\"\n",
    "\n",
    "margin = 20\n",
    "for bbox in normalized_bboxes:\n",
    "\n",
    "    # Get the bounding rectangle data:\n",
    "    x, y, w, h = bbox\n",
    "\n",
    "    # Estimate the bounding rect area:\n",
    "    rectArea = w * h\n",
    "\n",
    "    # Set a min area threshold\n",
    "    minArea = 10\n",
    "\n",
    "    # # Filter blobs by area:\n",
    "    if rectArea > minArea:\n",
    "\n",
    "        # Draw bounding box:\n",
    "        color = (0, 255, 0)\n",
    "        cv2.rectangle(im_copy, (int(x), int(y)),\n",
    "                      (int(x + w), int(y + h)), color, 2)\n",
    "        cv2.imshow(\"Bounding Boxes\", im_copy)\n",
    "\n",
    "        # Crop bounding box:\n",
    "        currentCrop = im_copy[max(0, y-margin):min(height, y+h+margin),max(0, x-margin):min(width, x+w+margin)]\n",
    "        p = model_predict(currentCrop)\n",
    "\n",
    "        eq += p\n",
    "        cv2.waitKey(0)\n",
    "\n",
    "print(eq)\n",
    "print(f\"{eq}={eval(eq)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea85459f-4ad4-49ba-8e22-fd1e877c183b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949fe61e-ce2f-4e6a-8b99-19292c4177ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31035a51-6dea-49a5-b183-246abe43bb28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
